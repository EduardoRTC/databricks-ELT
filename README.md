# ğŸ“Š Sales Performance Lakehouse

> Lakehouse moderno para anÃ¡lise de performance de vendas com detecÃ§Ã£o automÃ¡tica de anomalias

## ğŸ¯ Objetivo

Este projeto implementa um **Lakehouse** robusto que processa dados de vendas diÃ¡rias, combinando a flexibilidade de um Data Lake com a estrutura e performance de um Data Warehouse. Aplica transformaÃ§Ãµes inteligentes e regras de negÃ³cio para gerar insights acionÃ¡veis sobre performance comercial e detectar possÃ­veis irregularidades nos preÃ§os.

## ğŸ—ï¸ Arquitetura Lakehouse

### Medallion Architecture
```
ğŸ“ Landing Zone     â†’     ğŸ“ Raw Zone     â†’     ğŸ“ Bronze (Clean)     â†’     ğŸ“ Silver (Trusted)     â†’     ğŸ“ Gold (Analytics)
   Arquivo CSV           Parquet bruto        Limpeza bÃ¡sica          Regras de negÃ³cio        Dados para consumo
```

> **Lakehouse = Data Lake + Data Warehouse**  
> Combina a flexibilidade do armazenamento de dados nÃ£o estruturados com a performance e governanÃ§a de um Data Warehouse tradicional.

### Stack TecnolÃ³gica

| Componente | Tecnologia | Finalidade |
|------------|------------|------------|
| **Processamento** | PySpark | TransformaÃ§Ãµes distribuÃ­das dos dados |
| **Plataforma** | Databricks Lakehouse | Ambiente de execuÃ§Ã£o e orquestraÃ§Ã£o |
| **OrquestraÃ§Ã£o** | Databricks Jobs/Tasks/Pipelines | AutomaÃ§Ã£o da pipeline |
| **Armazenamento** | DBFS | Lakehouse para todas as camadas |
| **Formato** | Parquet + Delta Tables | Armazenamento ACID e otimizado |

## ğŸ“‹ Estrutura dos Dados

### Entrada (Landing)
```csv
venda_id,data,vendedor_id,vendedor_nome,produto_id,produto_nome,categoria,preco_unit,qtd,desconto,canal,cidade,estado,comentarios
V001,2024-01-15,123,JoÃ£o Silva,P001,Smartphone X,EletrÃ´nicos,899.90,2,0.10,ecommerce,SÃ£o Paulo,SP,lorem ipsum
```

### Raw Zone (Parquet)
```
Mesmo conteÃºdo da Landing, mas convertido para formato Parquet
- Melhor performance de leitura
- CompressÃ£o automÃ¡tica
- Preserva tipos de dados originais
```

### SaÃ­da (Gold)
```csv
venda_id,data,vendedor_id,produto_id,categoria,valor_total,desconto,score_venda,flag_anomalia
V001,2024-01-15,123,P001,EletrÃ´nicos,1619.82,0.10,VENDA_PREMIUM,FALSE
```

## ğŸ”„ Pipeline de TransformaÃ§Ã£o

### ğŸ“¥ Landing Zone - RecepÃ§Ã£o de Dados
- ğŸ“‚ Recebe arquivo CSV: `vendas_mock.csv`
- ğŸ·ï¸ Registra metadados do arquivo (nome, tamanho, timestamp)
- ğŸ“ Armazena em `/landing/yyyy/MM/dd/`

### ğŸ“¦ Raw Zone - ConversÃ£o para Parquet
- ğŸ”„ Move arquivo da Landing para Raw
- ğŸ“Š Converte CSV para formato Parquet (sem transformaÃ§Ãµes)
- ğŸ“ Salva em `/raw/yyyy/MM/dd/vendas_mock.parquet`
- âœ¨ MantÃ©m dados exatamente como recebidos

### ğŸ¥‰ Bronze Layer - Limpeza e PadronizaÃ§Ã£o
- âœ… RemoÃ§Ã£o de registros com campos crÃ­ticos nulos
- âœ… ConversÃ£o e validaÃ§Ã£o de tipos de dados
- âœ… PadronizaÃ§Ã£o para snake_case
- âœ… Logging de qualidade dos dados

### ğŸ¥ˆ Silver Layer - Regras de NegÃ³cio

#### ğŸ’° CÃ¡lculo de Valor Total
```python
valor_total = preco_unit * qtd * (1 - desconto)
```

#### ğŸ·ï¸ Score de ClassificaÃ§Ã£o
- **ALTO_RISCO**: Desconto > 20%
- **VENDA_PREMIUM**: Valor total > R$ 1.000
- **NORMAL**: Demais casos

#### ğŸš¨ DetecÃ§Ã£o de Anomalias
Identifica preÃ§os 30% acima/abaixo da mÃ©dia da categoria:
```python
if abs(preco_unit - preco_medio_categoria) / preco_medio_categoria > 0.30:
    flag_anomalia = True
```

### ğŸ¥‡ Gold Layer - Dados AnalÃ­ticos
- ğŸ“Š Dataset otimizado para BI e anÃ¡lises
- ğŸ—œï¸ Apenas campos essenciais
- ğŸ“ˆ Pronto para dashboards e relatÃ³rios

## ğŸ”§ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o

### PrÃ©-requisitos
- Databricks Lakehouse Workspace configurado
- Cluster com PySpark habilitado
- Acesso ao DBFS e Unity Catalog

### Estrutura do Projeto
```
project/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ process_file_to_parquet.py
â”‚   â”œâ”€â”€ bronze_clean.py
â”‚   â”œâ”€â”€ silver_trusted.py
â”‚   â””â”€â”€ gold_final.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ job.yaml              # ConfiguraÃ§Ã£o da task Databricks
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vendas_mock.csv    # Arquivo de exemplo
â””â”€â”€ README.md
```

### Deployment
1. **Upload dos notebooks** para o Databricks Workspace
2. **ConfiguraÃ§Ã£o do Job** usando o arquivo `job.yaml`:
   ```yaml
   name: "Lakehouse Sales Performance Pipeline"
   tasks:
     - task_key: "landing_to_raw"
       notebook_task:
         notebook_path: "/notebooks/00_landing_to_raw"
     - task_key: "raw_to_bronze"
       depends_on:
         - task_key: "landing_to_raw"
       notebook_task:
         notebook_path: "/notebooks/bronze_clean"
        ...
   ```

3. **Trigger** da execuÃ§Ã£o via *File Arrival*

## ğŸ“ˆ Monitoramento e Controle

### Tabela de Auditoria
Cada execuÃ§Ã£o registra mÃ©tricas de qualidade:

| MÃ©trica | DescriÃ§Ã£o |
|---------|-----------|
| `linhas_recebidas` | Total de registros no arquivo original |
| `linhas_validas` | Registros apÃ³s limpeza |
| `colunas_removidas` | Campos descartados na transformaÃ§Ã£o |
| `regras_aplicadas` | Regras de negÃ³cio executadas |
| `status` | Sucesso/Erro da execuÃ§Ã£o |

## ğŸ¯ Casos de Uso

### Para Gestores de Vendas
- Identificar vendedores com alta taxa de descontos
- Monitorar performance por categoria de produto
- Detectar padrÃµes sazonais nas vendas

### Para Auditoria
- Rastrear vendas com preÃ§os suspeitos
- Validar aplicaÃ§Ã£o de polÃ­ticas de desconto
- Gerar relatÃ³rios de conformidade

### Para Analistas de BI
- Dashboard de performance em tempo real
- AnÃ¡lises de tendÃªncia e forecasting
- SegmentaÃ§Ã£o de clientes e produtos

## ğŸš€ PrÃ³ximos Passos

- [ ] Implementar ML para detecÃ§Ã£o avanÃ§ada de fraudes
- [ ] Implementar alertas automÃ¡ticos para anomalias
- [ ] Implementar ML + Alertas anÃ¡lise em tempo real com Kafka

---

**Desenvolvido por Eduardo Carvalho**