{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Configuração\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "VOLUME  = \"elt_volume\"\n",
    "\n",
    "VOL_ROOT = PurePosixPath(\"/Volumes\") / CATALOG / SCHEMA / VOLUME\n",
    "SILVER_ROOT = VOL_ROOT / \"silver\"\n",
    "today_str = date.today().isoformat()\n",
    "time_str  = datetime.now().strftime(\"%H%M\")  # Current time: 1503\n",
    "SILVER_PART = SILVER_ROOT / today_str.replace(\"-\", \"/\")\n",
    "SILVER_BASE = SILVER_PART / f\"vendas_silver_processed_{time_str}\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Recupera caminho do bronze_csv da task anterior\n",
    "task_key = \"Execute_process_bronze_clean\"  # Nome real da task no Workflow\n",
    "try:\n",
    "    BRONZE_CSV = dbutils.jobs.taskValues.get(taskKey=task_key, key=\"bronze_csv\", debugValue=\"Not set\")\n",
    "    if not BRONZE_CSV or not dbutils.fs.ls(str(BRONZE_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em {BRONZE_CSV}\")\n",
    "except Exception as e:\n",
    "    # Fallback: Procurar o diretório mais recente criado pelo Bronze\n",
    "    BRONZE_ROOT = VOL_ROOT / \"bronze\"\n",
    "    bronze_base = str(BRONZE_ROOT / today_str.replace(\"-\", \"/\"))\n",
    "    bronze_files = dbutils.fs.ls(bronze_base)\n",
    "    BRONZE_CSV = None\n",
    "    if bronze_files:\n",
    "        bronze_dirs = sorted([f for f in bronze_files if f.isDir() and f.name.startswith(\"vendas_bronze_cleaned_\")], key=lambda x: x.modificationTime, reverse=True)\n",
    "        for sub_dir_entry in bronze_dirs:\n",
    "            sub_dir = sub_dir_entry.path.replace(\"dbfs:\", \"\")\n",
    "            sub_files = dbutils.fs.ls(sub_dir)\n",
    "            for sf in sub_files:\n",
    "                if sf.name.endswith(\".csv\"):\n",
    "                    BRONZE_CSV = sf.path.replace(\"dbfs:\", \"\")\n",
    "                    break\n",
    "            if BRONZE_CSV:\n",
    "                break\n",
    "    if not BRONZE_CSV or not dbutils.fs.ls(str(BRONZE_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em subdiretórios de {bronze_base}\")\n",
    "\n",
    "# Cria diretórios necessários\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "dbutils.fs.mkdirs(str(SILVER_PART))\n",
    "\n",
    "# Leitura do CSV da camada bronze\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(f\"dbfs:{BRONZE_CSV}\"))\n",
    "\n",
    "# Processamento: Geração de campos essenciais\n",
    "df_silver = (df\n",
    "    .withColumn(\"valor_liquido\", round(col(\"preco_unit\") * col(\"qtd\") * (1 - col(\"desconto\") / 100), 2))\n",
    "    .withColumn(\"score_venda\", col(\"valor_liquido\") / 1000)  # Exemplo de pontuação\n",
    "    .withColumn(\"flag_anomalia\", (col(\"valor_liquido\") > 5000).cast(\"integer\")))\n",
    "\n",
    "# Seleção de campos essenciais\n",
    "essential_columns = [\"venda_id\", \"data\", \"vendedor_id\", \"produto_id\", \"categoria\", \"valor_liquido\", \"desconto\", \"score_venda\", \"flag_anomalia\"]\n",
    "df_silver = df_silver.select(essential_columns)\n",
    "\n",
    "# Contagem de linhas processadas\n",
    "total_rows_processed = df_silver.count()\n",
    "\n",
    "# Salva o arquivo processado\n",
    "df_silver.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"dbfs:{SILVER_BASE}\")\n",
    "\n",
    "# Encontra o primeiro arquivo particionado para task value\n",
    "silver_files = dbutils.fs.ls(f\"dbfs:{SILVER_BASE}\")\n",
    "if not silver_files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo encontrado em {SILVER_BASE}\")\n",
    "silver_csv_path = [f.path.replace(\"dbfs:\", \"\") for f in silver_files if f.path.endswith(\".csv\")][0]\n",
    "\n",
    "# Registro de metadados\n",
    "metadata = {\n",
    "    \"total_rows_processed\": total_rows_processed,\n",
    "    \"output_path\": silver_csv_path\n",
    "}\n",
    "\n",
    "# Salva metadados como JSON\n",
    "metadata_path = SILVER_PART / f\"metadata_vendas_silver_{time_str}.json\"\n",
    "dbutils.fs.put(f\"dbfs:{metadata_path}\", str(metadata), overwrite=True)\n",
    "\n",
    "# Salva contextos para próximas tasks\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"silver_csv\", value=silver_csv_path)\n",
    "    dbutils.jobs.taskValues.set(key=\"silver_metadata\", value=str(metadata_path))\n",
    "except Exception as e:\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
