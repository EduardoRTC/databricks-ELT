{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c74c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração fixa\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "VOLUME  = \"elt_volume\"\n",
    "\n",
    "VOL_ROOT = PurePosixPath(\"/Volumes\") / CATALOG / SCHEMA / VOLUME\n",
    "LANDING  = VOL_ROOT / \"landing\"\n",
    "RAW_ROOT = VOL_ROOT / \"raw\"\n",
    "\n",
    "today_str = date.today().isoformat()\n",
    "time_str  = datetime.now().strftime(\"%H%M\")\n",
    "RAW_PART  = RAW_ROOT / today_str.replace(\"-\", \"/\") / f\"vendas_mock_{time_str}\"\n",
    "\n",
    "LANDING_CSV = LANDING / \"vendas_mock.csv\"\n",
    "RAW_CSV     = RAW_PART / \"vendas_mock.csv\"\n",
    "PARQUET_OUT = RAW_PART / f\"vendas_mock_{time_str}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Garante volume e diretórios\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "for p in [LANDING, RAW_ROOT, RAW_PART]:\n",
    "    dbutils.fs.mkdirs(str(p))\n",
    "\n",
    "# Valida arquivo em Landing\n",
    "if not dbutils.fs.ls(str(LANDING_CSV)):\n",
    "    raise FileNotFoundError(f\"Arquivo não encontrado em {LANDING_CSV}\")\n",
    "else:\n",
    "    print(f\"DEBUG: Arquivo encontrado em {LANDING_CSV}\")\n",
    "\n",
    "# Move CSV para Raw\n",
    "dbutils.fs.rm(str(RAW_CSV), recurse=True)\n",
    "dbutils.fs.mv(str(LANDING_CSV), str(RAW_CSV))\n",
    "\n",
    "# Converte para Parquet único\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(f\"dbfs:{RAW_CSV}\"))\n",
    "\n",
    "tmp_dir = str(RAW_PART / \"_tmp_parquet\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(f\"dbfs:{tmp_dir}\")\n",
    "\n",
    "part_file = [f.path for f in dbutils.fs.ls(f\"dbfs:{tmp_dir}\") if f.path.endswith(\".parquet\")][0]\n",
    "dbutils.fs.rm(f\"dbfs:{PARQUET_OUT}\", recurse=True)\n",
    "dbutils.fs.mv(part_file, f\"dbfs:{PARQUET_OUT}\")\n",
    "dbutils.fs.rm(f\"dbfs:{tmp_dir}\", recurse=True)\n",
    "\n",
    "# Salva contextos para próximos tasks\n",
    "task_key = \"Execute_process_file_to_parquet\"\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"raw_csv\", value=str(RAW_CSV))\n",
    "    dbutils.jobs.taskValues.set(key=\"raw_dir\", value=str(RAW_PART))\n",
    "    dbutils.jobs.taskValues.set(key=\"parquet\", value=str(PARQUET_OUT))\n",
    "except Exception as e:\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
