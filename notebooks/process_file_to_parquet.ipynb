{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c74c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração fixa\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "VOLUME  = \"elt_volume\"\n",
    "\n",
    "VOL_ROOT = PurePosixPath(\"/Volumes\") / CATALOG / SCHEMA / VOLUME\n",
    "LANDING  = VOL_ROOT / \"landing\"\n",
    "RAW_ROOT = VOL_ROOT / \"raw\"\n",
    "\n",
    "today_str = date.today().isoformat()\n",
    "time_str  = datetime.now().strftime(\"%H%M\")\n",
    "RAW_PART  = RAW_ROOT / today_str.replace(\"-\", \"/\") / f\"vendas_mock_{time_str}\"\n",
    "\n",
    "LANDING_CSV = LANDING / \"vendas_mock.csv\"\n",
    "RAW_CSV     = RAW_PART / \"vendas_mock.csv\"\n",
    "PARQUET_OUT = RAW_PART / f\"vendas_mock_{time_str}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Garante volume e diretórios\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "for p in [LANDING, RAW_ROOT, RAW_PART]:\n",
    "    dbutils.fs.mkdirs(str(p))\n",
    "    print(f\"DEBUG: Criado diretório {p}\")\n",
    "\n",
    "# Valida arquivo em Landing\n",
    "if not dbutils.fs.ls(str(LANDING_CSV)):\n",
    "    print(f\"DEBUG: Arquivo não encontrado em {LANDING_CSV}\")\n",
    "    raise FileNotFoundError(f\"Arquivo não encontrado em {LANDING_CSV}\")\n",
    "else:\n",
    "    print(f\"DEBUG: Arquivo encontrado em {LANDING_CSV}\")\n",
    "\n",
    "# Move CSV para Raw\n",
    "dbutils.fs.rm(str(RAW_CSV), recurse=True)\n",
    "dbutils.fs.mv(str(LANDING_CSV), str(RAW_CSV))\n",
    "print(f\"DEBUG: Arquivo movido de {LANDING_CSV} para {RAW_CSV}\")\n",
    "\n",
    "# Converte para Parquet único\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(f\"dbfs:{RAW_CSV}\"))\n",
    "print(f\"DEBUG: Arquivo CSV lido de {RAW_CSV}, total de linhas: {df.count()}\")\n",
    "\n",
    "tmp_dir = str(RAW_PART / \"_tmp_parquet\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(f\"dbfs:{tmp_dir}\")\n",
    "print(f\"DEBUG: Arquivo Parquet temporário salvo em {tmp_dir}\")\n",
    "\n",
    "part_file = [f.path for f in dbutils.fs.ls(f\"dbfs:{tmp_dir}\") if f.path.endswith(\".parquet\")][0]\n",
    "dbutils.fs.rm(f\"dbfs:{PARQUET_OUT}\", recurse=True)\n",
    "dbutils.fs.mv(part_file, f\"dbfs:{PARQUET_OUT}\")\n",
    "dbutils.fs.rm(f\"dbfs:{tmp_dir}\", recurse=True)\n",
    "print(f\"DEBUG: Arquivo Parquet final salvo em {PARQUET_OUT}\")\n",
    "\n",
    "# Salva contextos para próximos tasks\n",
    "task_key = \"Execute_process_file_to_parquet\"  # Nome real da task no Workflow\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"raw_csv\", value=str(RAW_CSV))\n",
    "    dbutils.jobs.taskValues.set(key=\"raw_dir\", value=str(RAW_PART))\n",
    "    dbutils.jobs.taskValues.set(key=\"parquet\", value=str(PARQUET_OUT))\n",
    "    print(f\"DEBUG: Task values successfully set - raw_csv: {str(RAW_CSV)}\")\n",
    "except Exception as e:\n",
    "    print(f\"DEBUG: Failed to set task values: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Debug: Verifica os valores salvos\n",
    "print(\"DEBUG: Task values set:\")\n",
    "print(f\"  raw_csv: {dbutils.jobs.taskValues.get(taskKey=task_key, key='raw_csv', debugValue='Not set')}\")\n",
    "print(f\"  raw_dir: {dbutils.jobs.taskValues.get(taskKey=task_key, key='raw_dir', debugValue='Not set')}\")\n",
    "print(f\"  parquet: {dbutils.jobs.taskValues.get(taskKey=task_key, key='parquet', debugValue='Not set')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
