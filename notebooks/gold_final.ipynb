{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "VOLUME  = \"elt_volume\"\n",
    "\n",
    "VOL_ROOT = PurePosixPath(\"/Volumes\") / CATALOG / SCHEMA / VOLUME\n",
    "GOLD_ROOT = VOL_ROOT / \"gold\"\n",
    "today_str = date.today().isoformat()\n",
    "time_str  = datetime.now().strftime(\"%H%M\")  # Current time: 1503\n",
    "GOLD_PART = GOLD_ROOT / today_str.replace(\"-\", \"/\")\n",
    "GOLD_BASE = GOLD_PART / f\"vendas_gold_final_{time_str}\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Recupera caminho do silver_csv da task anterior\n",
    "task_key = \"Execute_process_silver_clean\"  # Nome real da task no Workflow\n",
    "try:\n",
    "    SILVER_CSV = dbutils.jobs.taskValues.get(taskKey=task_key, key=\"silver_csv\", debugValue=\"Not set\")\n",
    "    print(f\"DEBUG: Retrieved silver_csv from task values: {SILVER_CSV}\")\n",
    "    if not SILVER_CSV or not dbutils.fs.ls(str(SILVER_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em {SILVER_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"DEBUG: Erro ao recuperar silver_csv: {str(e)}\")\n",
    "    # Fallback: Procurar o diretório mais recente criado pelo Silver\n",
    "    SILVER_ROOT = VOL_ROOT / \"silver\"\n",
    "    silver_base = str(SILVER_ROOT / today_str.replace(\"-\", \"/\"))\n",
    "    silver_files = dbutils.fs.ls(silver_base)\n",
    "    SILVER_CSV = None\n",
    "    if silver_files:\n",
    "        # Ordena por tempo de modificação para pegar o diretório mais recente\n",
    "        silver_dirs = sorted([f for f in silver_files if f.isDir() and f.name.startswith(\"vendas_silver_processed_\")], key=lambda x: x.modificationTime, reverse=True)\n",
    "        for sub_dir_entry in silver_dirs:\n",
    "            sub_dir = sub_dir_entry.path.replace(\"dbfs:\", \"\")\n",
    "            sub_files = dbutils.fs.ls(sub_dir)\n",
    "            for sf in sub_files:\n",
    "                if sf.name.endswith(\".csv\"):\n",
    "                    SILVER_CSV = sf.path.replace(\"dbfs:\", \"\")\n",
    "                    print(f\"DEBUG: Found candidate file in {sub_dir}: {SILVER_CSV}\")\n",
    "                    break\n",
    "            if SILVER_CSV:\n",
    "                break\n",
    "    if not SILVER_CSV or not dbutils.fs.ls(str(SILVER_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em subdiretórios de {silver_base}\")\n",
    "    print(f\"DEBUG: Fallback to constructed path: {SILVER_CSV}\")\n",
    "\n",
    "# Cria diretórios necessários\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "dbutils.fs.mkdirs(str(GOLD_PART))\n",
    "print(f\"DEBUG: Criado diretório {GOLD_PART}\")\n",
    "\n",
    "# Leitura do CSV da camada silver\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(f\"dbfs:{SILVER_CSV}\"))\n",
    "print(f\"DEBUG: Arquivo CSV lido de {SILVER_CSV}, total de linhas: {df.count()}\")\n",
    "\n",
    "# Seleção dos campos essenciais para a camada gold\n",
    "columns_to_keep = [\n",
    "    \"venda_id\",\n",
    "    \"data\",\n",
    "    \"vendedor_id\",\n",
    "    \"produto_id\",\n",
    "    \"categoria\",\n",
    "    \"valor_liquido\",\n",
    "    \"desconto\",\n",
    "    \"score_venda\",\n",
    "    \"flag_anomalia\"\n",
    "]\n",
    "df_gold = df.select(columns_to_keep)\n",
    "\n",
    "# Contagem de linhas processadas\n",
    "total_rows_processed = df_gold.count()\n",
    "print(f\"DEBUG: Total de linhas processadas: {total_rows_processed}\")\n",
    "\n",
    "# Salva o arquivo final em formato CSV\n",
    "df_gold.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"encoding\", \"UTF-8\").csv(f\"dbfs:{GOLD_BASE}\")\n",
    "print(f\"DEBUG: Arquivo salvo em {GOLD_BASE}\")\n",
    "\n",
    "# Encontra o primeiro arquivo particionado para task value\n",
    "gold_files = dbutils.fs.ls(f\"dbfs:{GOLD_BASE}\")\n",
    "if not gold_files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo encontrado em {GOLD_BASE}\")\n",
    "gold_csv_path = [f.path.replace(\"dbfs:\", \"\") for f in gold_files if f.path.endswith(\".csv\")][0]\n",
    "print(f\"DEBUG: Primeiro arquivo particionado encontrado: {gold_csv_path}\")\n",
    "\n",
    "# Registro de metadados\n",
    "metadata = {\n",
    "    \"total_rows_processed\": total_rows_processed,\n",
    "    \"output_path\": gold_csv_path\n",
    "}\n",
    "\n",
    "# Salva metadados como JSON\n",
    "metadata_path = GOLD_PART / f\"metadata_vendas_gold_{time_str}.json\"\n",
    "dbutils.fs.put(f\"dbfs:{metadata_path}\", str(metadata), overwrite=True)\n",
    "print(f\"DEBUG: Metadados salvos em {metadata_path}\")\n",
    "\n",
    "# Salva contextos para próximas tasks\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"gold_csv\", value=gold_csv_path)\n",
    "    dbutils.jobs.taskValues.set(key=\"gold_metadata\", value=str(metadata_path))\n",
    "    print(f\"DEBUG: Task values successfully set - gold_csv: {gold_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"DEBUG: Failed to set task values: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"DEBUG: Set gold_csv: {gold_csv_path}\")\n",
    "print(f\"DEBUG: Set gold_metadata: {str(metadata_path)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
