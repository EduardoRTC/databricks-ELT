{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pathlib import PurePosixPath\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "# Configuração\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "VOLUME  = \"elt_volume\"\n",
    "\n",
    "VOL_ROOT = PurePosixPath(\"/Volumes\") / CATALOG / SCHEMA / VOLUME\n",
    "BRONZE_ROOT = VOL_ROOT / \"bronze\"\n",
    "today_str = date.today().isoformat()\n",
    "time_str  = datetime.now().strftime(\"%H%M\") \n",
    "BRONZE_PART = BRONZE_ROOT / today_str.replace(\"-\", \"/\")\n",
    "BRONZE_BASE = BRONZE_PART / f\"vendas_bronze_cleaned_{time_str}\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Recupera caminho do raw_csv da task anterior\n",
    "task_key = \"Execute_process_file_to_parquet\"\n",
    "try:\n",
    "    RAW_CSV = dbutils.jobs.taskValues.get(taskKey=task_key, key=\"raw_csv\", debugValue=\"/Volumes/workspace/default/elt_volume/raw/2025/07/30/vendas_mock_1452/vendas_mock.csv\")\n",
    "    if not dbutils.fs.ls(str(RAW_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em {RAW_CSV}\")\n",
    "except Exception as e:\n",
    "    RAW_ROOT = VOL_ROOT / \"raw\"\n",
    "    raw_base = str(RAW_ROOT / today_str.replace(\"-\", \"/\"))\n",
    "    raw_files = dbutils.fs.ls(raw_base)\n",
    "    RAW_CSV = None\n",
    "    for f in raw_files:\n",
    "        if f.isDir():\n",
    "            sub_dir = f.path.replace(\"dbfs:\", \"\")\n",
    "            sub_files = dbutils.fs.ls(sub_dir)\n",
    "            for sf in sub_files:\n",
    "                if sf.name == \"vendas_mock.csv\":\n",
    "                    RAW_CSV = sf.path.replace(\"dbfs:\", \"\")\n",
    "                    break\n",
    "            if RAW_CSV:\n",
    "                break\n",
    "    if not RAW_CSV or not dbutils.fs.ls(str(RAW_CSV)):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado em subdiretórios de {raw_base}\")\n",
    "\n",
    "# Cria diretórios necessários\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "dbutils.fs.mkdirs(str(BRONZE_PART))\n",
    "\n",
    "# Leitura do CSV da camada raw\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(f\"dbfs:{RAW_CSV}\"))\n",
    "\n",
    "# Total de linhas recebidas\n",
    "total_rows_received = df.count()\n",
    "\n",
    "# Padronização de nomes de colunas para snake_case\n",
    "column_mapping = {\n",
    "    'venda_id': 'venda_id',\n",
    "    'data': 'data',\n",
    "    'vendedor_id': 'vendedor_id',\n",
    "    'vendedor_nome': 'vendedor_nome',\n",
    "    'produto_id': 'produto_id',\n",
    "    'produto_nome': 'produto_nome',\n",
    "    'categoria': 'categoria',\n",
    "    'preco_unit': 'preco_unit',\n",
    "    'qtd': 'qtd',\n",
    "    'desconto': 'desconto',\n",
    "    'canal': 'canal',\n",
    "    'cidade': 'cidade',\n",
    "    'estado': 'estado',\n",
    "    'comentarios': 'comentarios'\n",
    "}\n",
    "\n",
    "df = df.select([col(old_name).alias(new_name) for old_name, new_name in column_mapping.items()])\n",
    "\n",
    "# Identifica colunas com nulos antes da limpeza\n",
    "null_columns = [col_name for col_name in df.columns if df.filter(col(col_name).isNull()).count() > 0]\n",
    "\n",
    "# Exclusão de linhas com null em campos críticos\n",
    "critical_columns = ['venda_id', 'data', 'produto_id', 'vendedor_id', 'preco_unit', 'qtd']\n",
    "df_clean = df.dropna(subset=critical_columns)\n",
    "\n",
    "# Conversão de tipos\n",
    "df_clean = (df_clean\n",
    "    .withColumn('data', to_date(col('data')))\n",
    "    .withColumn('preco_unit', col('preco_unit').cast(FloatType()))\n",
    "    .withColumn('qtd', col('qtd').cast(IntegerType())))\n",
    "\n",
    "# Contagem de linhas válidas e inválidas\n",
    "total_rows_valid = df_clean.count()\n",
    "total_rows_invalid = total_rows_received - total_rows_valid\n",
    "\n",
    "# Salva o arquivo limpo\n",
    "df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"dbfs:{BRONZE_BASE}\")\n",
    "\n",
    "# Encontra o primeiro arquivo particionado para task value\n",
    "bronze_files = dbutils.fs.ls(f\"dbfs:{BRONZE_BASE}\")\n",
    "if not bronze_files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo encontrado em {BRONZE_BASE}\")\n",
    "bronze_csv_path = [f.path.replace(\"dbfs:\", \"\") for f in bronze_files if f.path.endswith(\".csv\")][0]\n",
    "\n",
    "# Registro de metadados\n",
    "metadata = {\n",
    "    \"total_rows_received\": total_rows_received,\n",
    "    \"total_rows_valid\": total_rows_valid,\n",
    "    \"total_rows_invalid\": total_rows_invalid,\n",
    "    \"null_columns_removed\": null_columns,\n",
    "    \"output_path\": bronze_csv_path\n",
    "}\n",
    "\n",
    "# Salva metadados como JSON\n",
    "metadata_path = BRONZE_PART / f\"metadata_vendas_bronze_{time_str}.json\"\n",
    "dbutils.fs.put(f\"dbfs:{metadata_path}\", str(metadata), overwrite=True)\n",
    "\n",
    "# Salva contextos para próximas tasks\n",
    "try:\n",
    "    dbutils.jobs.taskValues.set(key=\"bronze_csv\", value=bronze_csv_path)\n",
    "    dbutils.jobs.taskValues.set(key=\"bronze_metadata\", value=str(metadata_path))\n",
    "except Exception as e:\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
